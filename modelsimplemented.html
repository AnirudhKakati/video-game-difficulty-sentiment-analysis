<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Models Implemented - Player Sentiment Analysis in Video Games</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-okaidia.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>
</head>
    <style>
        ul {
            list-style-type: disc;
            margin-left: 20px; 
            padding-left: 10px; 
        }

        ul ul {
            list-style-type: circle; 
            margin-left: 20px; 
        }

        ul ul ul {
            list-style-type: square; 
        }

        li {
            margin: 5px 0;
        }
    </style>
<body style="background-image: url('images/background7.jpg');">
    <header>
        <h1>Player Sentiment Analysis in Video Games</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="introduction.html">Introduction</a></li>
                <li><a href="dataexploration.html">Data Exploration</a></li>
                <li><a href="modelsimplemented.html">Models Implemented</a></li>
                <li><a href="conclusion.html">Conclusion</a></li>
                <li><a href="team.html">Team</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <h2>Model Implementation</h2>

        <section id="BERT-model">
            <h3>1. BERT Transformer Model with “review” as input and “voted_up” as output</h3>
            <p>
                The BERT model (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer-based language model designed to understand text context bidirectionally. 
                We leveraged it for text classification by fine-tuning it on our dataset to predict whether a review was positive (voted_up)/
            </p>
            <p>
                We will first load in our featured_reviews.csv into a dataframe. We take our “review” feature as X or our input and take “voted_up” as y or our output. 
                The data initially looks like this:
            </p>
            <img src="images/model1.jpg" alt="Raw Review Image" class="data-image">
            <p>
                BERT model first tokenizes this text data before training. The tokenized transformed data looks like this:
            </p>
            <img src="images/model1.jpg" alt="Raw Review Image" class="data-image">
            <p>
                Our processing involves the following steps:
                <ul>
                    <li>Used BERT's pre-trained tokenizer (bert-base-uncased) to tokenize the review text</li>
                    <li>Each review was: <br>
                        <ul type="o">
                            <li>Split into tokens</li>
                            <li>Converted into unique token IDs (input_ids)</li>
                            <li>Padded or truncated to a maximum length of 128 tokens</li>
                            <li>Generated an attention_mask to indicate which tokens should be attended to</li>
                        </ul>
                    </li>
                    <li> Created a ReviewDataset class to manage the tokenized inputs and corresponding labels</li>
                    <li>Split the dataset into training and testing sets (X_train, y_train, X_test, y_test)</li>
                </ul>
            </p>
            <p>
                The model training involved the following:
                <ul>
                    <li>Architecture:<br>
                        <ul type="o">
                            <li>Fine-tuned the bert-base-uncased model for binary classification</li>
                            <li>Added a classification head on top of BERT to predict the voted_up label (output layer with sigmoid activation for probabilities)</li>
                        </ul>
                    </li>
                    <li>Training Configuration:<br>
                        <ul type="o">
                            <li>Optimized with AdamW (specific for transformers)</li>
                            <li>Loss function: Binary Cross-Entropy Loss</li>
                            <li>Metrics: Accuracy, F1-Score</li>
                            <li>Batch size: Adjusted to fit the GPU memory</li>
                            <li>Learning rate: Fine-tuned with a warm-up scheduler for stable convergence</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                After training the BERT model, we evaluated it on the test set. Below are the results:
            </p>
            <img src="images/model1.jpg" alt="Raw Review Image" class="data-image">
            <p>
                <ul>
                    <li>Accuracy: 93.44%</li>
                    <li>F1 Score: 96.36% (for class 1 positive reviews)</li>
                    <li>Classification Report: <br>
                        <ul type="o">
                            <li>Class 0 (Negative reviews): <br>
                                <ul>
                                    <li>Precision: 65%</li>
                                    <li>Recall: 69%</li>
                                    <li>F1 Score: 67%</li>
                                </ul>
                            </li>
                            <li>Class 1 (Positive reviews): <br>
                                <ul>
                                    <li>Precision: 97%</li>
                                    <li>Recall: 96%</li>
                                    <li>F1 Score: 96%</li>
                                </ul>
                            </li>
                            <li>Weighted Average: Precision = 94%, Recall = 93%, F1-Score = 94%</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                The model performs exceptionally well for positive reviews (class 1), which dominate the dataset. 
                Performance for negative reviews (class 0) is weaker, with lower precision and recall. 
                The overall weighted metrics indicate a robust model for predicting review positivity
            </p>
            <p>
                To try to improve our model we will incorporate more features into the next model
            </p>
            
        </section>
        
        <section id="data-preprocessing">
            <h3>Data Preprocessing</h3>
            <p>
                Reviews from each game were stored in JSON Files. 
                First, we compiled all JSON files into one single pandas dataframe for data preprocessing. 
                Any duplicate or non english records were removed. 
                A field called “author” was a dictionary, which was flattened by appending each of its keys as a separate column in the dataframe. 
                Records without “playtime_at_review” fields were dropped. For our analysis we would only need the columns: "game_name", "review", "voted_up", "timestamp_created", "author_num_games_owned", "author_num_reviews", "author_playtime_at_review", "author_playtime_last_two_weeks" and "author_playtime_forever".
                Only these relevant columns were kept with others getting discarded. 
                The "review" column was cleaned with all text being converted to lowercase for uniformity, punctuation and non alphanumeric characters were removed using regular expressions. 
                We also filter out reviews with lack of meaningful content. We do this by first checking if the review has "valid words".
                "Valid words" are defined to be meaningful english words. But some terms which are commonly used in gaming scenarios like "gg", "goty", "despawn", etc. are also added to our set of valid words.
                However, we cannot manually add all words relevant to the games and there could still be words which are specific to some games and their communities like the word "elden" used in the game Elden Ring.
                So, we add another check in our validation such that if a word, which isn't already marked valid by our inital check, appears more than 3 times across other reviews is marked as a valid words.
                With these checks, the reviews are marked as relevant only if atleast 50% of the words are found to be valid.
                All records with empty or irrelevant reviews after the transformations were dropped with the dataset now having close to 43000 records. 
                The cleaned dataset is stored in a csv which looks like this:
            </p>
            <img src="images/cleaned_dataset.jpg" alt="Cleaned Dataset Image" class="data-image">
            <p>For a detailed walkthrough of our data collection and preprocessing step, you can view our 
                full Jupyter notebook <a href="getting_and_cleaning_reviews.html" target="_blank">here</a>.
            </p>
        </section>

        <section id="visualizations">
            <h3>Inital Data Exploration and Visualizations</h3>
            <p>
                First we look at some inital data exploration. We load our cleaned reviews dataset into a pandas dataframe df. The columns along with their data types look like this:
            </p>
            <img src="images/columns_and_datatypes.jpg" alt="Cleaned Dataset Image" class="data-image">
            <p>
                We have 9 columns, with game_name and the review being objects (string), the voted_up which indicates if the review was positive or negative being boolean,
                and all our columns with information about the author of the review being numerical.
                We can then get a summary of all numerical columns of our dataset:
            </p>
            <img src="images/numerical_summary.jpg" alt="Cleaned Dataset Image" class="data-image">
            <p>
                Some common statistics like the mean, standard deviation and other percentile values for all our numerical columns are displayed.
                We can then see how many positive and negative reviews we have in total in our dataset.
            </p>
            <img src="images/positive_negative_count.jpg" alt="Cleaned Dataset Image" class="data-image">
            <p>
                There around 38600 positive reviews and 4300 negative reviews.
                Now lets look at some specific columns which are of importance to us. Lets check the average review length.
            </p>
            <img src="images/average_review_length.jpg" alt="Cleaned Dataset Image" class="data-image">
            <p>
                We can see that people use around 172 characters on average in their reviews. 
                We'll look at the average playtime across all the games in hours played next.
            </p>
            <img src="images/average_playtime_hours.jpg" alt="Cleaned Dataset Image" class="data-image">
            <p>
                The average playtime of the reviewed games is around 185 hours. This seems very high. Does the average gamer really spend this much time on a game?
                Since our review data includes some multiplayer games where players tend to invest much more time, the data can get skewed.
                Let us look at median playtime of the reviewed games in hours played.
            </p>
            <img src="images/median_playtime_hours.jpg" alt="Cleaned Dataset Image" class="data-image">
            <p>
                The median comes out to be around 35 hours. This is more representative of how much time gamers usually spend on a game.
                Lets check if there's any correlation between playing time and review length.
            </p>
            <img src="images/playtime_review_length_corr.jpg" alt="Cleaned Dataset Image" class="data-image">
            <p>
                No significant correlation can be found between the review length and time spent playing that game. 
                So its not like people who play games for longer write longer reviews.
            </p>
            <p>
                Now its time for us to look at some visualizations to a get a better look at our data.
            </p>
            <div class="visualization">
                <h4> Median Playtime (in hours) by Game </h4>
                <p> 
                    First let us look at what the median playtime for each game looks like.
                </p>
                <img src="graphs/playtime_by_game.png" alt="Median Playtime" class="data-image">
                <p>
                    As one would expect, multiplayer PVP games like DOTA 2, Rocket League and Counter Strike 2 have the highest playtimes, where as single player and CO-OP titles like INSIDE, and Portal 2 have shorter playtimes.
                    Call of Duty Modern Warfare has both multiplayer and singleplayer options but still has low playtimes, which might indicate that people tend to drop the game after playing the singleplayer campaign and don't indulge too much into multiplayer.
                </p>
                
                <h4> Average Review Length by Game </h4>
                <p> 
                    Let us see how long the average review is for each game.
                </p>
                <img src="graphs/review_length_by_game.png" alt="Review length" class="data-image">
                <p>
                    Pillars of Eternity, Returnal and The Witness seem to have the longest reviews while Counter Strike 2 and DOTA 2 have the shortest. 
                    Multiplayers games tend to have shorter reviews compared to the others.
                </p>
                
                <h4> Review Sentiment Distribution by Game </h4>
                <p> 
                    We explore the distribution of positive and negative reviews for each game.
                </p>
                <img src="graphs/review_sentiment_by_game.png" alt="Review Sentiment" class="data-image">
                <p>
                    It seems that multiplayer games like Rocket League, Team Fortress 2 and Counter Strike 2 have a higher ratio of negative reviews than other single player and CO-OP games. 
                </p>

                <h4> Percentage of Positive Reviews by Game </h4>
                <p> 
                    If we want to explore review sentiment of the games further we can see the sorted percentages of positive reviews for the games.
                </p>
                <img src="graphs/percentage_positive_reviews.png" alt="Positive Sentiment" class="data-image">
                <p>
                    Portal 2, Subnautica and Factorio have the highest percentage of positive reviews wherease Rocket League, Team Fortress 2 and Counter Strike 2 have the least.
                </p>

                <h4> Review Length vs Playtime </h4>
                <p> 
                    We can visualize the distribution of the length of the reviews and the playtime while writing the review.
                </p>
                <img src="graphs/review_length_vs_playtime.png" alt="Length vs Sentiment" class="data-image">
                <p>
                    Its hard to identify any specific relationship between the two. But we can also see that both the reviews length and playtime tend to be on the shorter side.
                    We have too many data points and its hard to see the proper distribution. So we'll take a sample of 1000 reviews and then plot them next.
                </p>

                <h4> Review Length vs Playtime (Sampled and Zoomed) </h4>
                <p> 
                    We take a sample of 1000 random reviews from our set to get a clearer visual. We also limit our playtime from 0 to 2000 hours and review length from o to 1000 characters as thats where most data points lie.
                </p>
                <img src="graphs/review_length_vs_playtime_sampled_data.png" alt="Length vs Sentiment zoomed" class="data-image">
                <p>
                    We can see most playtimes are under 250 hours and most reviews are also under 200 characters. People tend to write short reviews for most games they play.
                </p>

                <h4> Distribution of When Reviews are Posted </h4>
                <p> 
                    The ratio of playtime at review to the total playtime can give us an idea about when people tend to post their reviews. 
                    A small ratio means the reviews were posted very early on when they started playing the game and bigger ratios would mean the reviews were posted closer to the end of their gameplay.
                </p>
                <img src="graphs/distribution_of_playtime_ratio.png" alt="Playtime ratio distribution" class="data-image">
                <p>
                    We can see most highest ratios are more common. So most gamers tend to put their reviews closer to the end of their gameplay. 
                    This means they are more likely to post the reviews when they have completed the game or they dont play it anymore.
                    Very few people tend to post reviews of games as they're playing them.
                </p>

                <h4> Word Cloud of Most frequent Words </h4>
                <p> 
                    Let us visualize what kind of words are most frequently used in our reviews.
                </p>
                <img src="graphs/word_cloud.png" alt="wordcloud" class="data-image">
                <p>
                    Words like "game", "play", "one", "time", "good", "fun", etc. are very commonly used across our reviews. 
                    But we have predominantly positive reviews so words used in those will overshadow words used in negative ones.
                    Let us see if the kind of words used changes across positive and negative reviews
                </p>

                <h4> Word Cloud of Most frequent Words across Positive Reviews</h4>
                <p> 
                    First, let us visualize what kind of words are most frequently used in positive reviews.
                </p>
                <img src="graphs/wordcloud_positive.png" alt="wordcloud positive" class="data-image">
                <p>
                    Words like "game", "play", "time", "good", "one", "fun" are very common. These are the same ones which appeared in our overall word cloud. 
                    Some more words like "great" and "love" also start appearing more frequently. 
                </p>

                <h4> Word Cloud of Most frequent Words across Negative Reviews</h4>
                <p> 
                    Next, let us visualize what kind of words are most frequently used in negative reviews.
                </p>
                <img src="graphs/wordcloud_negative.png" alt="wordcloud negative" class="data-image">
                <p>
                    In negative reviews, words like "game", "play", "time", "one" are still the most common. Words like "great" and "love" become much less frequent.
                </p>

                <h4> Review Length Distribution by Review Sentiment</h4>
                <p> 
                    The review length for each type of sentiment is analyzed next.
                </p>
                <img src="graphs/review_length_by_sentiment.png" alt="review length by sentiment" class="data-image">
                <p>
                    We limit our review length to upto 1000 characters as we have lot of outliers.
                    Positive Reviews tend to have much lesser median number of charcaters. 
                    Negative reviews tend to be longer with a wider interquartile range.
                </p>

                <h4> Playtime distribution by Review Sentiment</h4>
                <p> 
                    We now explore the distribution of playtime of two sentiments of reviews.
                </p>
                <img src="graphs/playtime_by_sentiment.png" alt="playtime by sentiment" class="data-image">
                <p>
                    Both types of reviews have a lot of outliers so we limit our playtime to upto 1000.
                    Positive Reviews tend to have a higher median playtime compared to negative reviews. 
                    However negative reviews have a much wider interquartile range of playtimes.
                </p>

                <h4> Frequency of Positive and Negative Reviews with Playtime</h4>
                <p> 
                    Finally, let us see the frequency of negative and positive reviews for different ranges of playtime.
                </p>
                <img src="graphs/review_sentiment_by_playtime.png" alt="playtime by sentiment" class="data-image">
                <p>
                    Shorter playtimes have a higher ratio of negative to positive reviews compared to longer reviews. 
                    As evident from the median, people tend to play for shorter times when they leave a negative review.
                    The playtimes between 1000 to 5000 minutes has the most number of reviews. 
                </p>
                
            </div>
            <p>For a detailed walkthrough of our data exploration process and visualizations, you can view our 
                full Jupyter notebook <a href="initial_data_exploration_and_visualization.html" target="_blank">here</a>.
            </p>
        </section>

    </main>

    <footer>
        2024 Video Game Sentiment Analysis Team
    </footer>
</body>
</html>
