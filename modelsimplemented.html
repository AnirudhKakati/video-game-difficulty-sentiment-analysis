<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Models Implemented - Player Sentiment Analysis in Video Games</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-okaidia.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>
</head>
    <style>
        ul {
            list-style-type: disc;
            margin-left: 20px; 
            padding-left: 10px; 
        }

        ul ul {
            list-style-type: circle; 
            margin-left: 20px; 
        }

        ul ul ul {
            list-style-type: square; 
        }

        li {
            margin: 5px 0;
        }
    </style>
<body style="background-image: url('images/background8.jpg');">
    <header>
        <h1>Player Sentiment Analysis in Video Games</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="introduction.html">Introduction</a></li>
                <li><a href="dataexploration.html">Data Exploration</a></li>
                <li><a href="modelsimplemented.html">Models Implemented</a></li>
                <li><a href="conclusion.html">Conclusion</a></li>
                <li><a href="team.html">Team</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <h2>Model Implementation</h2>

        <section id="BERT-model">
            <h3>1. BERT Transformer Model with “review” as input and “voted_up” as output</h3>
            <p>
                The BERT model (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer-based language model designed to understand text context bidirectionally. 
                We leveraged it for text classification by fine-tuning it on our dataset to predict whether a review was positive (voted_up).
            </p>
            <p>
                We will first load in our featured_reviews.csv into a dataframe. We take our “review” feature as X or our input and take “voted_up” as y or our output. 
                The data initially looks like this:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                BERT model first tokenizes this text data before training. The tokenized transformed data looks like this:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                Our processing involves the following steps:
                <ul>
                    <li>Used BERT's pre-trained tokenizer (bert-base-uncased) to tokenize the review text</li>
                    <li>Each review was: <br>
                        <ul type="o">
                            <li>Split into tokens</li>
                            <li>Converted into unique token IDs (input_ids)</li>
                            <li>Padded or truncated to a maximum length of 128 tokens</li>
                            <li>Generated an attention_mask to indicate which tokens should be attended to</li>
                        </ul>
                    </li>
                    <li> Created a ReviewDataset class to manage the tokenized inputs and corresponding labels</li>
                    <li>Split the dataset into training and testing sets (X_train, y_train, X_test, y_test)</li>
                </ul>
            </p>
            <p>
                The model training involved the following:
                <ul>
                    <li>Architecture:<br>
                        <ul type="o">
                            <li>Fine-tuned the bert-base-uncased model for binary classification</li>
                            <li>Added a classification head on top of BERT to predict the voted_up label (output layer with sigmoid activation for probabilities)</li>
                        </ul>
                    </li>
                    <li>Training Configuration:<br>
                        <ul type="o">
                            <li>Optimized with AdamW (specific for transformers)</li>
                            <li>Loss function: Binary Cross-Entropy Loss</li>
                            <li>Metrics: Accuracy, F1-Score</li>
                            <li>Batch size: Adjusted to fit the GPU memory</li>
                            <li>Learning rate: Fine-tuned with a warm-up scheduler for stable convergence</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                After training the BERT model, we evaluated it on the test set. Below are the results:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                <ul>
                    <li>Accuracy: 93%</li>
                    <li>F1 Score: 96% (for class 1 positive reviews)</li>
                    <li>Classification Report: <br>
                        <ul type="o">
                            <li>Class 0 (Negative reviews): <br>
                                <ul>
                                    <li>Precision: 65%</li>
                                    <li>Recall: 69%</li>
                                    <li>F1 Score: 67%</li>
                                </ul>
                            </li>
                            <li>Class 1 (Positive reviews): <br>
                                <ul>
                                    <li>Precision: 97%</li>
                                    <li>Recall: 96%</li>
                                    <li>F1 Score: 96%</li>
                                </ul>
                            </li>
                            <li>Weighted Average: Precision = 94%, Recall = 93%, F1-Score = 94%</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                The model performs exceptionally well for positive reviews (class 1), which dominate the dataset. 
                Performance for negative reviews (class 0) is weaker, with lower precision and recall. 
                The overall weighted metrics indicate a robust model for predicting review positivity
            </p>
            <p>
                To try to improve our model we will incorporate more features into the next model.
            </p>
            
        </section>
        
        <section id="BERT model 2">
            <h3>2. BERT Transformer Model with “review”, “review_length” and “sentiment_score” as input and “voted_up” as output</h3>
            <p>
                Text reviews (review) contain detailed feedback, but their raw form is difficult for machine learning models to interpret directly. 
                By extracting a sentiment score, we provide a numerical representation of the review’s polarity, making it easier for the model to process
            </p>
            <p>
                VADER (Valence Aware Dictionary and sEntiment Reasoner) is a pre-trained rule-based sentiment analysis tool from the NLTK library, designed to handle social media text and other short reviews
            </p>
            <p>
                It provides four sentiment scores for a given text:
                <uL>
                    <li>Positive: Proportion of positive words</li>
                    <li>Neutral: Proportion of neutral words</li>
                    <li>Negative: Proportion of negative words</li>
                    <li>Compound: A single aggregated score that represents the overall sentiment of the text</li>
                </uL>
            </p>
            <p>
                For each review in the review column, we applied sia.polarity_scores(x) to compute sentiment scores. 
                We selected the compound score, which is a normalized value between -1 (most negative) and 1 (most positive). 
                This value is stored as the new column “sentiment_score”.
            </p>
            <p>
                We take “review”, “review_length” and “sentiment_score” as our input X and “voted_up” as our output y. Our data looks like this:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                The numerical columns are normalized by min-max scaling. The data now looks like this:
            <p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                The processing steps are similar to the previous model: <br>
                <ul>
                    <li>Used the BertTokenizer to tokenize the review text into input_ids and attention_mask</li>
                    <li>Created a custom Dataset class (ReviewDataset) to include: <br>
                        <ul>
                            <li>Text Features: Tokenized input IDs and attention masks</li>
                            <li>Numerical Features: Normalized review_length and sentiment_score</li>
                            <li>Labels: voted_up</li>
                        </ul>
                            
                    </li>
                    <li>The key difference is that the __getitem__ method was updated to return both numerical features and tokenized text features, 
                        making the dataset compatible with the combined BERT model</li>
                </ul>
            </p>
            <p>
                To incorporate both text and numerical features, a custom BERT-based model was designed:
                <ul>
                    <li>Base Model: Used the pre-trained bert-base-uncased model. 
                        Extracted the CLS token's representation (768 dimensions) from the BERT output as a summary of the review's meaning</li>
                    <li>Numerical Features Integration: <br>
                        <ul>
                            <li>Concatenated the CLS token output with the normalized numerical features</li>
                            <li>Adjusted the fully connected layer to accept an input size of 768 + len(numerical_cols) (to account for the numerical features)</li>
                        </ul>
                    </li>
                    <li>Classification Layer: <br>
                        <ul>
                            <li>Used a fully connected layer with 2 output neurons (for binary classification)</li>
                            <li>Added a dropout layer for regularization</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                The model was trained similar to the previous model:
                <ul>
                    <li>Optimized using the AdamW optimizer with a learning rate of 2e-5</li>
                    <li>Used Cross-Entropy Loss as the objective function</li>
                    <li>Trained over 3 epochs, with a batch size of 16</li>
                    <li>The key difference being that the forward pass now included the concatenated CLS token and numerical features</li>
                </ul>
            </p>
            <p>
                After training this new BERT model, we evaluated it on the test set. Below are the results:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                <ul>
                    <li>Accuracy: 93%</li>
                    <li>F1 Score: 96% (for class 1 positive reviews)</li>
                    <li>Classification Report: <br>
                        <ul type="o">
                            <li>Class 0 (Negative reviews): <br>
                                <ul>
                                    <li>Precision: 69%</li>
                                    <li>Recall: 63%</li>
                                    <li>F1 Score: 66%</li>
                                </ul>
                            </li>
                            <li>Class 1 (Positive reviews): <br>
                                <ul>
                                    <li>Precision: 96%</li>
                                    <li>Recall: 97%</li>
                                    <li>F1 Score: 96%</li>
                                </ul>
                            </li>
                            <li>Weighted Average: Precision = 93%, Recall = 93%, F1-Score = 93%</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                Adding the additional features (review_length and sentiment_score) did not improve the model's performance. 
                Overall, while there was a minor gain in precision for negative reviews, the drop in recall led to no significant improvement in the overall metrics.
            </p>
        </section>
        <section id="LightGBM Classifier Model">
            <h3>3. LightGBM Classifier Model with “review”, “review_length” and “sentiment_score” as input and “voted_up” as output</h3>
        </section>
        <section id="Ensemble LightGBM, LR, NB Model">
            <h3>4. Ensemble Stacking Model using LightGBM, Logistic Regression and Naïve Bayes with “review”, “review_length” and “sentiment_score” as input and “voted_up” as output</h3>
        </section>
        <section id="Bagging with LightGBM Model ">
            <h3>5. Bagging Model using LightGBM Classifier with “review”, “review_length” and “sentiment_score” as input and “voted_up” as output</h3>
        </section>
        <section id="LightGBM Classifier Model 2">
            <h3>6. LightGBM Model with “review_length”, “sentiment_score” as input and “mentions_difficulty” as output</h3>
        </section>
        <section id="XGBoost Classifier Model ">
            <h3>7. XGBoost Classifier Model with “review_length”, “sentiment_score” as input and “mentions_difficulty” as output</h3>
        </section>
        <section id="Ensemble XGBoost, LR, NB Model ">
            <h3>8. Ensemble Stacking Model using XGBoost Classifier, Logistic Regression and Naïve Bayes with “review_length”, “sentiment_score” as input and “mentions_difficulty” as output</h3>
        </section>
        <section id="Ensemble LightGBM, LR, NB Model 2">
            <h3>9. Ensemble Stacking Model using LightGBM, Logistic Regression and Naïve Bayes with “review_length”, “sentiment_score” as input and “mentions_difficulty” as output</h3>
        </section>
        <section id="DT Classifier Model ">
            <h3>10. Decision Tree Model with genre data as input and "mentions_difficulty" as output</h3>
        </section>
        <section id="RF Classifier Model ">
            <h3>11. Random Forest Model with genre data as input and "mentions_difficulty" as output</h3>
        </section>
        <section id="LR Classifier Model ">
            <h3>12. Logistic Regression Model with genre data as input and "mentions_difficulty" as output</h3>
        </section>
    </main>

    <footer>
        2024 Video Game Sentiment Analysis Team
    </footer>
</body>
</html>
