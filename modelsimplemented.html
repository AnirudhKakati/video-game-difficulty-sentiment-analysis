<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Models Implemented - Player Sentiment Analysis in Video Games</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-okaidia.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>
</head>
    <style>
        ul {
            list-style-type: disc;
            margin-left: 20px; 
            padding-left: 10px; 
        }

        ul ul {
            list-style-type: circle; 
            margin-left: 20px; 
        }

        ul ul ul {
            list-style-type: square; 
        }

        li {
            margin: 5px 0;
        }
    </style>
<body style="background-image: url('images/background8.jpg');">
    <header>
        <h1>Player Sentiment Analysis in Video Games</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="introduction.html">Introduction</a></li>
                <li><a href="dataexploration.html">Data Exploration</a></li>
                <li><a href="modelsimplemented.html">Models Implemented</a></li>
                <li><a href="conclusion.html">Conclusion</a></li>
                <li><a href="team.html">Team</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <h2>Model Implementation</h2>

        <section id="BERT-model">
            <h3>1. BERT Transformer Model with “review” as input and “voted_up” as output</h3>
            <p>
                The BERT model (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer-based language model designed to understand text context bidirectionally. 
                We leveraged it for text classification by fine-tuning it on our dataset to predict whether a review was positive (voted_up).
            </p>
            <p>
                We will first load in our featured_reviews.csv into a dataframe. We take our “review” feature as X or our input and take “voted_up” as y or our output. 
                The data initially looks like this:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                BERT model first tokenizes this text data before training. The tokenized transformed data looks like this:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                Our processing involves the following steps:
                <ul>
                    <li>Used BERT's pre-trained tokenizer (bert-base-uncased) to tokenize the review text</li>
                    <li>Each review was: <br>
                        <ul type="o">
                            <li>Split into tokens</li>
                            <li>Converted into unique token IDs (input_ids)</li>
                            <li>Padded or truncated to a maximum length of 128 tokens</li>
                            <li>Generated an attention_mask to indicate which tokens should be attended to</li>
                        </ul>
                    </li>
                    <li> Created a ReviewDataset class to manage the tokenized inputs and corresponding labels</li>
                    <li>Split the dataset into training and testing sets (X_train, y_train, X_test, y_test)</li>
                </ul>
            </p>
            <p>
                The model training involved the following:
                <ul>
                    <li>Architecture:<br>
                        <ul type="o">
                            <li>Fine-tuned the bert-base-uncased model for binary classification</li>
                            <li>Added a classification head on top of BERT to predict the voted_up label (output layer with sigmoid activation for probabilities)</li>
                        </ul>
                    </li>
                    <li>Training Configuration:<br>
                        <ul type="o">
                            <li>Optimized with AdamW (specific for transformers)</li>
                            <li>Loss function: Binary Cross-Entropy Loss</li>
                            <li>Metrics: Accuracy, F1-Score</li>
                            <li>Batch size: Adjusted to fit the GPU memory</li>
                            <li>Learning rate: Fine-tuned with a warm-up scheduler for stable convergence</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                After training the BERT model, we evaluated it on the test set. Below are the results:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                <ul>
                    <li>Accuracy: 93%</li>
                    <li>F1 Score: 96% (for class 1 positive reviews)</li>
                    <li>Classification Report: <br>
                        <ul type="o">
                            <li>Class 0 (Negative reviews): <br>
                                <ul>
                                    <li>Precision: 65%</li>
                                    <li>Recall: 69%</li>
                                    <li>F1 Score: 67%</li>
                                </ul>
                            </li>
                            <li>Class 1 (Positive reviews): <br>
                                <ul>
                                    <li>Precision: 97%</li>
                                    <li>Recall: 96%</li>
                                    <li>F1 Score: 96%</li>
                                </ul>
                            </li>
                            <li>Weighted Average: Precision = 94%, Recall = 93%, F1-Score = 94%</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                The model performs exceptionally well for positive reviews (class 1), which dominate the dataset. 
                Performance for negative reviews (class 0) is weaker, with lower precision and recall. 
                The overall weighted metrics indicate a robust model for predicting review positivity
            </p>
            <p>
                To try to improve our model we will incorporate more features into the next model.
            </p>
            
        </section>
        
        <section id="BERT model 2">
            <h3>2. BERT Transformer Model with “review”, “review_length” and “sentiment_score” as input and “voted_up” as output</h3>
            <p>
                Text reviews (review) contain detailed feedback, but their raw form is difficult for machine learning models to interpret directly. 
                By extracting a sentiment score, we provide a numerical representation of the review’s polarity, making it easier for the model to process
            </p>
            <p>
                VADER (Valence Aware Dictionary and sEntiment Reasoner) is a pre-trained rule-based sentiment analysis tool from the NLTK library, designed to handle social media text and other short reviews
            </p>
            <p>
                It provides four sentiment scores for a given text:
                <uL>
                    <li>Positive: Proportion of positive words</li>
                    <li>Neutral: Proportion of neutral words</li>
                    <li>Negative: Proportion of negative words</li>
                    <li>Compound: A single aggregated score that represents the overall sentiment of the text</li>
                </uL>
            </p>
            <p>
                For each review in the review column, we applied sia.polarity_scores(x) to compute sentiment scores. 
                We selected the compound score, which is a normalized value between -1 (most negative) and 1 (most positive). 
                This value is stored as the new column “sentiment_score”.
            </p>
            <p>
                We take “review”, “review_length” and “sentiment_score” as our input X and “voted_up” as our output y. Our data looks like this:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                The numerical columns are normalized by min-max scaling. The data now looks like this:
            <p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                The processing steps are similar to the previous model: <br>
                <ul>
                    <li>Used the BertTokenizer to tokenize the review text into input_ids and attention_mask</li>
                    <li>Created a custom Dataset class (ReviewDataset) to include: <br>
                        <ul>
                            <li>Text Features: Tokenized input IDs and attention masks</li>
                            <li>Numerical Features: Normalized review_length and sentiment_score</li>
                            <li>Labels: voted_up</li>
                        </ul>
                            
                    </li>
                    <li>The key difference is that the __getitem__ method was updated to return both numerical features and tokenized text features, 
                        making the dataset compatible with the combined BERT model</li>
                </ul>
            </p>
            <p>
                To incorporate both text and numerical features, a custom BERT-based model was designed:
                <ul>
                    <li>Base Model: Used the pre-trained bert-base-uncased model. 
                        Extracted the CLS token's representation (768 dimensions) from the BERT output as a summary of the review's meaning</li>
                    <li>Numerical Features Integration: <br>
                        <ul>
                            <li>Concatenated the CLS token output with the normalized numerical features</li>
                            <li>Adjusted the fully connected layer to accept an input size of 768 + len(numerical_cols) (to account for the numerical features)</li>
                        </ul>
                    </li>
                    <li>Classification Layer: <br>
                        <ul>
                            <li>Used a fully connected layer with 2 output neurons (for binary classification)</li>
                            <li>Added a dropout layer for regularization</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                The model was trained similar to the previous model:
                <ul>
                    <li>Optimized using the AdamW optimizer with a learning rate of 2e-5</li>
                    <li>Used Cross-Entropy Loss as the objective function</li>
                    <li>Trained over 3 epochs, with a batch size of 16</li>
                    <li>The key difference being that the forward pass now included the concatenated CLS token and numerical features</li>
                </ul>
            </p>
            <p>
                After training this new BERT model, we evaluated it on the test set. Below are the results:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                <ul>
                    <li>Accuracy: 93%</li>
                    <li>F1 Score: 96% (for class 1 positive reviews)</li>
                    <li>Classification Report: <br>
                        <ul type="o">
                            <li>Class 0 (Negative reviews): <br>
                                <ul>
                                    <li>Precision: 69%</li>
                                    <li>Recall: 63%</li>
                                    <li>F1 Score: 66%</li>
                                </ul>
                            </li>
                            <li>Class 1 (Positive reviews): <br>
                                <ul>
                                    <li>Precision: 96%</li>
                                    <li>Recall: 97%</li>
                                    <li>F1 Score: 96%</li>
                                </ul>
                            </li>
                            <li>Weighted Average: Precision = 93%, Recall = 93%, F1-Score = 93%</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                Adding the additional features (review_length and sentiment_score) did not improve the model's performance. 
                Overall, while there was a minor gain in precision for negative reviews, the drop in recall led to no significant improvement in the overall metrics.
            </p>
        </section>
        
        <section id="LightGBM Classifier Model">
            <h3>3. LightGBM Classifier Model with “review”, “review_length” and “sentiment_score” as input and “voted_up” as output</h3>
            <p>
                We will now use a LightGBM classifier model to try to model the same relationship. The data looks like this again:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                The numerical columns are again normalized and the data looks like this:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                The review data is transformed for it to work with our classifier model. 
                The transformation involves converting raw review text into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency). 
                This step allows text data to be represented numerically for machine learning models. The steps are:
                <uL>
                    <li>Tokenization: The TfidfVectorizer splits each review into individual words (tokens)</li>
                    <li>Vocabulary Creation: Builds a vocabulary of the most frequent 5000 words across all reviews</li>
                    <li>TF-IDF Score Calculation: Assigns a score to each word based on its frequency in the review (Term Frequency) and its rarity across all reviews (Inverse Document Frequency)</li>
                    <li>Sparse Matrix Representation: Converts each review into a row of numerical values (TF-IDF scores), where: <br>
                        <ul>
                            <li>Rows = Reviews</li>
                            <li>Columns = Words in the vocabulary</li>
                            <li>Values = Importance of the word in that review</li>
                        </ul>
                    </li>
                </uL>
            </p>
            <p>
                The transformed data looks like this:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                We used LightGBM (LGBMClassifier), a gradient boosting framework that is highly efficient for handling large datasets and sparse matrices (like our TF-IDF + numerical features). 
                The model was initialized with the parameter class_weight="balanced" to handle class imbalance between positive (voted_up=1) and negative (voted_up=0) reviews.
            </p>
            <p>
                The model was trained on the combined feature set of:
                <ul>
                    <li>TF-IDF matrix: Captures the textual content of the review</li>
                    <li>Scaled numerical features: review_length and sentiment_score</li>
                </ul>
            </p>
            <p>
                Hyperparameter Tuning: To improve the model’s performance, we conducted a Grid Search over a reduced parameter space to find the best combination of hyperparameters. 
                The following parameters were tuned:
                <uL>
                    <li>learning_rate: Controls how much the model adjusts weights after each boosting round</li>
                    <li>n_estimators: The number of boosting rounds</li>
                    <li>max_depth: The depth of each decision tree</li>
                    <li>subsample: The proportion of samples used for training each tree</li>
                    <li>colsample_bytree: The proportion of features used for training each tree</li>
                </uL>
                We used GridSearchCV with 3-fold cross-validation and optimized for the F1-score, balancing precision and recall
            </p>
            <p>
                Threshold Adjustment: In the best parameter model, we adjusted the decision threshold (default is 0.5) to optimize performance for different class priorities.
                The steps were the following:
                <ul>
                    <li>Obtained prediction probabilities using model.predict_proba</li>
                    <li>Tested thresholds ranging from 0.1 to 0.6</li>
                    <li>Evaluated metrics (precision, recall, F1-score) for each threshold</li>
                </ul>
                By lowering the threshold we improved recall for class 0 (negative reviews), reducing false negatives. 
                But by increasing the threshold precision for class 1 (positive reviews) improved, which reduced false positives.
                To balance both out, a threshold value of 0.2 was chosen.
            </p>
            <p>
                The results before hyperparameter tuning and threshold adjustment are the following:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                <ul>
                    <li>Accuracy: 87%</li>
                    <li>F1 Score: 88% (Weighted Average)</li>
                    <li>Classification Report: <br>
                        <ul type="o">
                            <li>Class 0 (Negative reviews): <br>
                                <ul>
                                    <li>Precision: 40%</li>
                                    <li>Recall: 76%</li>
                                    <li>F1 Score: 53%</li>
                                </ul>
                            </li>
                            <li>Class 1 (Positive reviews): <br>
                                <ul>
                                    <li>Precision: 97%</li>
                                    <li>Recall: 88%</li>
                                    <li>F1 Score: 92%</li>
                                </ul>
                            </li>
                            <li>Weighted Average: Precision = 92%, Recall = 87%, F1-Score = 88%</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                After tuning and threshold adjust, the following were the results:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                <ul>
                    <li>Accuracy: 91%</li>
                    <li>F1 Score: 91% (Weighted Average)</li>
                    <li>Classification Report: <br>
                        <ul type="o">
                            <li>Class 0 (Negative reviews): <br>
                                <ul>
                                    <li>Precision: 56%</li>
                                    <li>Recall: 56%</li>
                                    <li>F1 Score: 56%</li>
                                </ul>
                            </li>
                            <li>Class 1 (Positive reviews): <br>
                                <ul>
                                    <li>Precision: 95%</li>
                                    <li>Recall: 95%</li>
                                    <li>F1 Score: 95%</li>
                                </ul>
                            </li>
                            <li>Weighted Average: Precision = 91%, Recall = 91%, F1-Score = 91%</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                After hyperparameter tuning and threshold adjustment, the LightGBM model showed notable improvements in balancing precision and recall for Class 0 (negative reviews). 
                Precision and recall for Class 0 improved to 56%, raising the F1-score from 53% to 56%. Class 1 (positive reviews) maintained strong performance, with an F1-score of 95%, and the overall accuracy increased from 87% to 91%. 
                While the model successfully enhanced detection of negative reviews, the F1-score for Class 0 remains moderate due to class imbalance. The macro-average F1-score improved to 76%, but weighted F1-score showed minimal change. 
                These results indicate that the tuning effectively balanced performance between classes, though further techniques might be required to address residual challenges in negative review detection
            </p>
        </section>

        <section id="Ensemble LightGBM, LR, NB Model">
            <h3>4. Ensemble Stacking Model using LightGBM, Logistic Regression and Naïve Bayes with “review”, “review_length” and “sentiment_score” as input and “voted_up” as output</h3>
            <p>
                We try to use an ensemble model for modelling the same relationship. The dataset is the same which looks like this after scaling:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                The review is transformed again in a similar way as the last LightGBM model and it again looks like this:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                Ensemble models combine predictions from multiple base models to improve overall performance by leveraging the strengths of each model. 
                We implemented a stacking ensemble model, which:
                <uL>
                    <li>Combines predictions from multiple base models</li>
                    <li>Uses a meta-model (final estimator) to learn from these predictions and make the final decision</li>
                </uL>
                Our base models were the following:
                <uL>
                    <li> LightGBM Classifier <br>
                        <ul>
                            <li>A gradient boosting model, good for handling sparse data and complex patterns</li>
                            <li>Parameters: class_weight='balanced', random_state=42</li>
                        </ul>
                    </li>
                    <li> Logistic Regression <br>
                        <ul>
                            <li>A simple linear model, effective for linearly separable data</li>
                            <li>Parameters: max_iter=1000</li>
                        </ul>
                    </li>
                    <li> Multinomial NB
                        <ul>
                            <li>A probabilistic model, particularly suited for text data</li>
                            <li>Works well when features (e.g., TF-IDF) are conditionally independent</li>
                        </ul>
                    </li>
                </uL>
                We used Logistic Regression as the meta-model, which learns how to combine predictions from the base models. 
                Cross-validation (cv=3) ensures that the meta-model is trained on out-of-sample predictions
            </p>
            <p>
                Each base model was trained on the training set. 
                Predictions from these models were combined into new features for the meta-model.
                The meta-model was trained on the combined predictions to make the final decision
            </p>
            <p>
                Hyperparameter Tuning: To further optimize the stacking model, we performed hyperparameter tuning using grid search. 
                The grid search used 3-fold cross-validation and was optimized for the weighted F1-score. The following following parameters were tuned:
                <ul>
                    <li> LightGBM Parameters<br>
                        <ul>
                            <li>n_estimators: Number of boosting rounds ([100, 150])</li>
                            <li>learning_rate: Step size for weight updates ([0.05, 0.1])</li>
                            <li>max_depth: Depth of each tree ([3, 5])</li>
                        </ul>
                    </li>
                    <li>Logistic Regression Parameters (Meta-Model): <br>
                        <ul>
                            <li>C: Regularization strength ([0.1, 1.0]).</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                We will also do threshold adjustment from 0.1 to 0.9. A threshold value of 0.8 gave the most optimal results so we choose this.
            </p>
            <p>
                Before hyperparameter tuning and threshold adjustment we got the following results:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                <ul>
                    <li>Accuracy: 93%</li>
                    <li>F1 Score: 93% (Weighted Average)</li>
                    <li>Classification Report: <br>
                        <ul type="o">
                            <li>Class 0 (Negative reviews): <br>
                                <ul>
                                    <li>Precision: 75%</li>
                                    <li>Recall: 51%</li>
                                    <li>F1 Score: 61%</li>
                                </ul>
                            </li>
                            <li>Class 1 (Positive reviews): <br>
                                <ul>
                                    <li>Precision: 95%</li>
                                    <li>Recall: 98%</li>
                                    <li>F1 Score: 96%</li>
                                </ul>
                            </li>
                            <li>Weighted Average: Precision = 93%, Recall = 93%, F1-Score = 93%</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                After tuning and choosing threshold of 0.7, we got the following results:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                <ul>
                    <li>Accuracy: 93%</li>
                    <li>F1 Score: 93% (Weighted Average)</li>
                    <li>Classification Report: <br>
                        <ul type="o">
                            <li>Class 0 (Negative reviews): <br>
                                <ul>
                                    <li>Precision: 65%</li>
                                    <li>Recall: 62%</li>
                                    <li>F1 Score: 63%</li>
                                </ul>
                            </li>
                            <li>Class 1 (Positive reviews): <br>
                                <ul>
                                    <li>Precision: 96%</li>
                                    <li>Recall: 96%</li>
                                    <li>F1 Score: 96%</li>
                                </ul>
                            </li>
                            <li>Weighted Average: Precision = 93%, Recall = 93%, F1-Score = 93%</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                The stacking ensemble model initially performed well, achieving a weighted F1-score of 93% and high precision and recall for Class 1 (positive reviews). 
                However, Class 0 (negative reviews) struggled with a low recall of 51% and an F1-score of 61%, indicating room for improvement in identifying negative reviews. 
                After hyperparameter tuning and adjusting the decision threshold to 0.7, the model showed a better balance for Class 0, with precision improving from 75% to 65% and recall increasing from 51% to 62%. Although the F1-score for Class 0 rose marginally to 63%, the performance for Class 1 remained robust, with a precision and recall of 96%. Overall accuracy remained consistent at 93%, and the macro-average F1-score improved slightly, indicating that threshold adjustment effectively enhanced the model's ability to identify negative reviews without sacrificing the performance for positive reviews
            </p>
        </section>
        
        <section id="Bagging with LightGBM Model ">
            <h3>5. Bagging Model using LightGBM Classifier with “review”, “review_length” and “sentiment_score” as input and “voted_up” as output</h3>
            <p>
                We make an ensemble model by bagging the LightGBM Classifier and try to model the same relationship. The dataset is the same which looks like this after scaling:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                The review is transformed again in a similar way as the last ensemble model and it again looks like this:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                Bagging (Bootstrap Aggregating) combines multiple instances of a base model to reduce variance and improve stability. 
                Each base model is trained on a random subset of the data (with replacement), and predictions are aggregated to produce the final result.
            </p>
            <p>
                We implemented a BaggingClassifier using LightGBM as the base estimator. It has the following key parameters:
                <uL>
                    <li>Base Estimator: LightGBM (class_weight='balanced', random_state=42) to handle class imbalance and capture complex relationships</li>
                    <li> Bagging Parameters: <br>
                        <ul>
                            <li>n_estimators: Number of base models</li>
                            <li>max_samples: Fraction of training samples used by each base model</li>
                            <li>max_features: Fraction of features used by each base model</li>
                        </ul>
                    </li>
                </uL>
            </p>
            <p>
                To optimize the BaggingClassifier, we performed hyperparameter tuning using grid search on a small search space to maintain efficiency. 
                The grid search used 3-fold cross-validation and was optimized for the weighted F1-score. All 3 parameters are tuned:
                <uL>
                    <li>n_estimators: [5, 10]</li>
                    <li>max_samples: [0.6, 0.8]</li>
                    <li>max_features: [0.6, 0.8] </li>
                </uL>
            </p>
            <p>
                After training the BaggingClassifier with the best parameters, 
                we will also do threshold adjustment from 0.1 to 0.9. 
                A threshold value of 0.4 gave the most optimal results so we choose this.
            </p>
            <p>
                Before hyperparameter tuning and threshold adjustment we got the following results:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                <ul>
                    <li>Accuracy: 88%</li>
                    <li>F1 Score: 89% (Weighted Average)</li>
                    <li>Classification Report: <br>
                        <ul type="o">
                            <li>Class 0 (Negative reviews): <br>
                                <ul>
                                    <li>Precision: 43%</li>
                                    <li>Recall: 74%</li>
                                    <li>F1 Score: 55%</li>
                                </ul>
                            </li>
                            <li>Class 1 (Positive reviews): <br>
                                <ul>
                                    <li>Precision: 97%</li>
                                    <li>Recall: 89%</li>
                                    <li>F1 Score: 93%</li>
                                </ul>
                            </li>
                            <li>Weighted Average: Precision = 92%, Recall = 88%, F1-Score = 89%</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                After tuning and choosing threshold of 0.4, we got the following results:
            </p>
            <img src="images/model1.jpg" alt="Image" class="data-image">
            <p>
                <ul>
                    <li>Accuracy: 91%</li>
                    <li>F1 Score: 91% (Weighted Average)</li>
                    <li>Classification Report: <br>
                        <ul type="o">
                            <li>Class 0 (Negative reviews): <br>
                                <ul>
                                    <li>Precision: 55%</li>
                                    <li>Recall: 58%</li>
                                    <li>F1 Score: 57%</li>
                                </ul>
                            </li>
                            <li>Class 1 (Positive reviews): <br>
                                <ul>
                                    <li>Precision: 95%</li>
                                    <li>Recall: 95%</li>
                                    <li>F1 Score: 95%</li>
                                </ul>
                            </li>
                            <li>Weighted Average: Precision = 91%, Recall = 91%, F1-Score = 91%</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <p>
                The BaggingClassifier model initially performed reasonably well, achieving an accuracy of 88% and a weighted F1-score of 89%. However, Class 0 (negative reviews) had a low F1-score of 55% due to its relatively low precision (43%) and moderate recall (74%). After hyperparameter tuning and adjusting the threshold to 0.4, the model showed an improvement in balancing performance for Class 0. Precision for Class 0 increased to 55%, while recall descreased to 58%, resulting in a better F1-score of 57%. The performance for Class 1 (positive reviews) remained strong, with a precision and recall of 95%. 
                The overall accuracy increased to 91%, and the macro-average F1-score rose to 76%, reflecting a better balance between the two classes. 
            </p>
            
        </section>






        
        <section id="LightGBM Classifier Model 2">
            <h3>6. LightGBM Model with “review_length”, “sentiment_score” as input and “mentions_difficulty” as output</h3>
        </section>
        <section id="XGBoost Classifier Model ">
            <h3>7. XGBoost Classifier Model with “review_length”, “sentiment_score” as input and “mentions_difficulty” as output</h3>
        </section>
        <section id="Ensemble XGBoost, LR, NB Model ">
            <h3>8. Ensemble Stacking Model using XGBoost Classifier, Logistic Regression and Naïve Bayes with “review_length”, “sentiment_score” as input and “mentions_difficulty” as output</h3>
        </section>
        <section id="Ensemble LightGBM, LR, NB Model 2">
            <h3>9. Ensemble Stacking Model using LightGBM, Logistic Regression and Naïve Bayes with “review_length”, “sentiment_score” as input and “mentions_difficulty” as output</h3>
        </section>
        <section id="DT Classifier Model ">
            <h3>10. Decision Tree Model with genre data as input and "mentions_difficulty" as output</h3>
        </section>
        <section id="RF Classifier Model ">
            <h3>11. Random Forest Model with genre data as input and "mentions_difficulty" as output</h3>
        </section>
        <section id="LR Classifier Model ">
            <h3>12. Logistic Regression Model with genre data as input and "mentions_difficulty" as output</h3>
        </section>
    </main>

    <footer>
        2024 Video Game Sentiment Analysis Team
    </footer>
</body>
</html>
